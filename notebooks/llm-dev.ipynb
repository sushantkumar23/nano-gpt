{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an LLM (Large Language Model) from scratch\n",
    "\n",
    "In this notebook, we will build a character-level LLM from scratch using PyTorch. We will start with a Bigram model and then extend it to a Transformer model. We will use the text from \"Tiny Shakespeare\" dataset for training. The model will be able to generate text character by character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening and reading the text from the file\n",
    "with open('input.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Total number of characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let sample the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique characters: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# Let's find all the characters that are there in the text\n",
    "chars = sorted(list(set(text)))\n",
    "print(f\"Total number of unique characters: {len(chars)}\")\n",
    "print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# Let's now write an encode and decode function\n",
    "itos = { i: c for i, c in enumerate(chars) }\n",
    "stoi = { c: i for i, c in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: torch.Size([1115394])\n",
      "Data type: torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# Now let's encode the first 1000 characters\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"Shape of data: {data.shape}\")\n",
    "print(f\"Data type: {data.dtype}\")\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003854, 111540\n",
      "train_data type: torch.int64\n",
      "val_data type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"{len(train_data)}, {len(val_data)}\")\n",
    "print(f\"train_data type: {train_data.dtype}\")\n",
    "print(f\"val_data type: {val_data.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18]) -> Target: 47\n",
      "Context: tensor([18, 47]) -> Target: 56\n",
      "Context: tensor([18, 47, 56]) -> Target: 57\n",
      "Context: tensor([18, 47, 56, 57]) -> Target: 58\n",
      "Context: tensor([18, 47, 56, 57, 58]) -> Target: 1\n",
      "Context: tensor([18, 47, 56, 57, 58,  1]) -> Target: 15\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15]) -> Target: 47\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> Target: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "\n",
    "x = data[:block_size]\n",
    "y = data[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f\"Context: {context} -> Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of xb:  torch.Size([4, 8])\n",
      "Shape of yb:  torch.Size([4, 8])\n",
      "xb:  tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "yb:  tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "Context: [0] -> Target: 43\n",
      "Context: [0] -> Target: 58\n",
      "Context: [0] -> Target: 5\n",
      "Context: [0] -> Target: 57\n",
      "Context: [0] -> Target: 1\n",
      "Context: [0] -> Target: 46\n",
      "Context: [0] -> Target: 43\n",
      "Context: [0] -> Target: 39\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(block_size):\n\u001b[0;32m---> 23\u001b[0m         context \u001b[38;5;241m=\u001b[39m x[b, :i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     24\u001b[0m         target \u001b[38;5;241m=\u001b[39m y[b, i]\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Target: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size=4\n",
    "\n",
    "def get_batch(split, batch_size=4):\n",
    "    if split == \"train\":\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    ix = torch.randint(len(train_data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"Shape of xb: \", xb.shape)\n",
    "print(\"Shape of yb: \", yb.shape)\n",
    "print(\"xb: \", xb)\n",
    "print(\"yb: \", yb)\n",
    "\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for i in range(block_size):\n",
    "        context = x[b, :i+1]\n",
    "        target = y[b, i]\n",
    "        print(f\"Context: {context.tolist()} -> Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # x.shape = (B, T)\n",
    "        # targets.shape = (B)\n",
    "\n",
    "        # (B, T) -> (B, T, C)\n",
    "        logits = self.embedding(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B*T, C), targets.view(B*T))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, x, n):\n",
    "\n",
    "        # x.shape = (B, T)\n",
    "        for _ in range(n):\n",
    "            # (B, T) -> (B, T, C)\n",
    "            logits, _ = self(x)\n",
    "            # (B, T, C) -> (B, T, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # (B, T, C) -> (B, C)\n",
    "            probs = probs[:, -1, :]\n",
    "            # (B, C) -> (B)\n",
    "            x_next = torch.multinomial(probs, num_samples=1)\n",
    "            # cat[(B), (B, T)] -> (B, T+1)\n",
    "            x = torch.cat([x, x_next], dim=-1)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.vocab_size: 65\n",
      "logits.shape: torch.Size([2, 1, 65])\n",
      "loss: 5.7610015869140625\n",
      "probs.shape: torch.Size([2, 1, 65])\n",
      "probs: tensor([[[0.0031, 0.0027, 0.0027, 0.0202, 0.0086, 0.0411, 0.0016, 0.0050,\n",
      "          0.0161, 0.0392, 0.0191, 0.0415, 0.0249, 0.0070, 0.0014, 0.0046,\n",
      "          0.0736, 0.0260, 0.0204, 0.0149, 0.0181, 0.0019, 0.0034, 0.0248,\n",
      "          0.0165, 0.0202, 0.0174, 0.0123, 0.0023, 0.0050, 0.0246, 0.0068,\n",
      "          0.0045, 0.0419, 0.0252, 0.0334, 0.0107, 0.0158, 0.0264, 0.0061,\n",
      "          0.0037, 0.0075, 0.0050, 0.0219, 0.0027, 0.0102, 0.0359, 0.0069,\n",
      "          0.0060, 0.0021, 0.0325, 0.0049, 0.0027, 0.0100, 0.0337, 0.0139,\n",
      "          0.0133, 0.0053, 0.0070, 0.0102, 0.0323, 0.0047, 0.0152, 0.0044,\n",
      "          0.0166]],\n",
      "\n",
      "        [[0.0031, 0.0027, 0.0027, 0.0202, 0.0086, 0.0411, 0.0016, 0.0050,\n",
      "          0.0161, 0.0392, 0.0191, 0.0415, 0.0249, 0.0070, 0.0014, 0.0046,\n",
      "          0.0736, 0.0260, 0.0204, 0.0149, 0.0181, 0.0019, 0.0034, 0.0248,\n",
      "          0.0165, 0.0202, 0.0174, 0.0123, 0.0023, 0.0050, 0.0246, 0.0068,\n",
      "          0.0045, 0.0419, 0.0252, 0.0334, 0.0107, 0.0158, 0.0264, 0.0061,\n",
      "          0.0037, 0.0075, 0.0050, 0.0219, 0.0027, 0.0102, 0.0359, 0.0069,\n",
      "          0.0060, 0.0021, 0.0325, 0.0049, 0.0027, 0.0100, 0.0337, 0.0139,\n",
      "          0.0133, 0.0053, 0.0070, 0.0102, 0.0323, 0.0047, 0.0152, 0.0044,\n",
      "          0.0166]]], grad_fn=<SoftmaxBackward0>)\n",
      "Sum of probs: 2.0\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "print(f\"m.vocab_size: {m.vocab_size}\")\n",
    "\n",
    "# Let's pass the first batch\n",
    "b = 2\n",
    "idx = torch.zeros((b, 1), dtype=torch.long)\n",
    "logits, loss = m(idx)\n",
    "print(f\"logits.shape: {logits.shape}\")\n",
    "\n",
    "# Calculate the loss\n",
    "targets = torch.zeros((b), dtype=torch.long)\n",
    "loss = F.cross_entropy(logits[:, -1, :], targets)\n",
    "print(f\"loss: {loss}\")\n",
    "\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "# Shape should be [b, 1, vocab_size]\n",
    "print(f\"probs.shape: {probs.shape}\")\n",
    "# For manual inspection if the probs are positive and sum to 1\n",
    "print(f\"probs: {probs}\")\n",
    "\n",
    "# Probs should sum to 1\n",
    "print(f\"Sum of probs: {probs.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_probs.shape: torch.Size([2, 65])\n",
      "x_next.shape: torch.Size([2, 1])\n",
      "x_next: tensor([[46],\n",
      "        [34]])\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the distribution\n",
    "# The last element of the sequence is the one that we are interested in\n",
    "last_probs = probs[:, -1, :]\n",
    "print(f\"last_probs.shape: {last_probs.shape}\")\n",
    "x_next = torch.multinomial(input=last_probs, num_samples=1)\n",
    "print(f\"x_next.shape: {x_next.shape}\")\n",
    "print(f\"x_next: {x_next}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "giWkcz3bTv?luMIKuBlwJDQhrM3qTQT:zhHHycj:VK?CEFv\n",
      "x.?:?kCrAiIS:vB\n",
      "YnUr&my3niX.nw&fddP$qqDBqcefCMvtAJDfCcbDMX3kwXc.MJjaoMeXlC?gV.uHznpYc!JTdJ\n",
      "aF'cMW.,EAGThcnuDgDgm'zUFS$YFvFeAjbGI!$lZEJVhRE IGfTduAcsZcA$\n"
     ]
    }
   ],
   "source": [
    "# Generating using the model's generate method\n",
    "x = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# Untrained model will generate random characters\n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "print(decode((m.generate(x, n=200)[0]).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training BigramLanguageModel\n",
    "\n",
    "Now, we will try to train a Bigram language model. The model will predict the next character based on the previous character only. In an autoregressive manner, we will keep feeding the generated character back to the model to predict the next character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape: torch.Size([32, 8])\n",
      "yb.shape: torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "vocab_size = len(chars)\n",
    "learning_rate = 1e-3\n",
    "max_iters = 100\n",
    "eval_iters = 10\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "xb, yb = get_batch(\"train\", batch_size=batch_size)\n",
    "print(f\"xb.shape: {xb.shape}\")\n",
    "print(f\"yb.shape: {yb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and the optimizer\n",
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "optimizer = AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4350764751434326\n"
     ]
    }
   ],
   "source": [
    "max_iters = 10000\n",
    "\n",
    "for step in range(max_iters):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch(\"train\", batch_size=batch_size)\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "So, with just BigramLanguageModel, we were able to get the loss down to ~2.5. The model is able to create some structure in the text. But, it is not able to generate meaningful text. We will now try to build a Transformer model to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A:\n",
      "Anen.\n",
      "\n",
      "IUENTo heles, he y onthene h mas:\n",
      "Jut hyoiththe s yor at y, he, foonghefico, isinor h thed ach o tatheonkicalarck'ean, ththe n, IToo-ld t ld y,\n",
      "I ey, ite st tmerule th a tapan neme tat d,\n",
      "ANGowe.\n",
      "aceagulimar VINGowe watheyoutofolos choerr.\n",
      "Ke, hencu ld milb, wonelole,\n",
      "whesp IOMESen O, co s vo bertharoy,\n",
      "Torot hallanoth y'sher t hon,\n",
      "Paver asto? pp f hed tin, histard, tome t t\n",
      "NGr eacomaselldesh o?\n",
      "\n",
      "QUMAnd ingend stat me; me! pe mantyser my; ivencofon 'de\n",
      "Wavete be se'lfat, s D:\n",
      "AREThe\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), n=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
