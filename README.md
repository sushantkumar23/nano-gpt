## Implementation of Transformer

Building a GPT-like decoder-only transformer from scratch using PyTorch. The transformer is implemented in `transformer_model.py` and the training can be done by setting the `training` flag to `True` in `transformer_model.py`.

Things to do:

Phase 1:
[] Self-Attention
[] Scaled Dot-Product Attention
[] Feed-Forward Network
[] Positional Embedding
[] Multi-Head Attention
[] Normalization

Phase 2:
[] Residual Connection
[] Rotary Positional Embedding
[] RMS Layer Normalization
[] KV Cache
[] Multi-Query Attention
[] SwiGLU Activation (Feed-Forward Network)
