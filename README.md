## Implementation of Transformer

Building a GPT-like decoder-only transformer from scratch using PyTorch. The transformer is implemented in `model.py` and the training can be done by setting the `training` flag to `True` in `model.py`.

### Phase 1

- [x] Self-Attention
- [x] Scaled Dot-Product Attention
- [x] Feed-Forward Network
- [x] Positional Embedding
- [x] Residual Connection
- [ ] Multi-Head Attention
- [ ] Layer Normalization

### Phase 2

- [ ] Rotary Positional Embedding
- [ ] RMS Layer Normalization
- [ ] KV Cache
- [ ] Multi-Query Attention
- [ ] SwiGLU Activation (Feed-Forward Network)
