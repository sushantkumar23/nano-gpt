## Implementation of Transformer

Building a GPT-like decoder-only transformer from scratch using PyTorch. The transformer is implemented in `transformer_model.py` and the training can be done by setting the `training` flag to `True` in `transformer_model.py`.

### Phase 1

- [x] Self-Attention
- [x] Scaled Dot-Product Attention
- [x] Feed-Forward Network
- [x] Positional Embedding
- [ ] Multi-Head Attention
- [ ] Normalization

### Phase 2

- [ ] Residual Connection
- [ ] Rotary Positional Embedding
- [ ] RMS Layer Normalization
- [ ] KV Cache
- [ ] Multi-Query Attention
- [ ] SwiGLU Activation (Feed-Forward Network)
